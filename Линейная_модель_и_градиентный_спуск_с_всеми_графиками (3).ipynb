{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a\n\n\u042d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442, \u043a\u0430\u043a \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0438 \u043a\u0430\u043a \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0435\u0451 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Define a simple Logistic Regression with Gradient Descent\nclass LogisticRegressionGD:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = 0\n        self.loss_history = []\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        m, n = X.shape\n        self.weights = np.zeros(n)\n\n        # Gradient Descent\n        for i in range(self.n_iterations):\n            z = np.dot(X, self.weights) + self.bias\n            y_pred = self.sigmoid(z)\n\n            # Compute gradients\n            d_w = -np.dot(X.T, (y - y_pred)) / m\n            d_b = -np.sum(y - y_pred) / m\n\n            # Update weights and bias\n            self.weights -= self.learning_rate * d_w\n            self.bias -= self.learning_rate * d_b\n\n            # Track the loss\n            loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n            self.loss_history.append(loss)\n\n    def predict(self, X):\n        z = np.dot(X, self.weights) + self.bias\n        return self.sigmoid(z)\n\n# Function to plot the loss curve\ndef plot_loss_curve(loss_history, label):\n    plt.plot(range(1, len(loss_history) + 1), loss_history, label=label)\n    plt.xlabel('Iterations')\n    plt.ylabel('Loss')\n    plt.title('Loss Curve during Gradient Descent')\n    plt.legend()\n    plt.show()\n"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "\n# Now let's proceed with loading and preprocessing data\nfrom sklearn.datasets import make_classification\n\n# Example: Create a synthetic classification dataset\nX, y = make_classification(n_samples=100, n_features=5, n_informative=3, n_classes=2, random_state=42)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize the Logistic Regression model and fit it\nmodel = LogisticRegressionGD(learning_rate=0.01, n_iterations=1000)\nmodel.fit(X_train_scaled, y_train)\n\n# Plot the loss curve\nplot_loss_curve(model.loss_history, \"Logistic Regression\")\n"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "\n# Plot feature coefficients for Logistic Regression model\ndef plot_feature_coefficients(weights, label):\n    plt.barh(range(len(weights)), weights, color='lightblue')\n    plt.xlabel('Coefficient Value')\n    plt.title(f'Feature Coefficients: {label}')\n    plt.show()\n\n# Plot the coefficients for our logistic regression model\nplot_feature_coefficients(model.weights, \"Logistic Regression\")\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "\n# Plot ROC Curve\ndef plot_roc_curve(fpr, tpr, auc_score, label):\n    plt.plot(fpr, tpr, color='blue', label=f'{label} (AUC = {auc_score:.2f})')\n    plt.plot([0, 1], [0, 1], linestyle='--', color='red')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve: {label}')\n    plt.legend(loc='lower right')\n    plt.show()\n\n# Generate ROC curve for the model\ny_pred = model.predict(X_test_scaled)\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\nplot_roc_curve(fpr, tpr, roc_auc, \"Logistic Regression\")\n"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "\n# Plot Precision-Recall curve\ndef plot_precision_recall_curve(precision, recall, label):\n    plt.plot(recall, precision, color='green', label=f'{label}')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(f'Precision-Recall Curve: {label}')\n    plt.legend()\n    plt.show()\n\n# Generate Precision-Recall curve for the model\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nplot_precision_recall_curve(precision, recall, \"Logistic Regression\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Conclusion\n\nThis notebook demonstrates the training of a logistic regression model using gradient descent. The graphs show how the model learns, evaluates, and how different features contribute to the prediction."}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 5}